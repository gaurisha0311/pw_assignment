{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bfe95d6-dd01-4109-82f8-3746d6007c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPrecision and recall are two important metrics used to evaluate the performance of classification models, \\n\\nespecially in scenarios where class imbalance exists.\\n\\nPrecision: Precision measures the accuracy of positive predictions made by the model. It is the ratio of true\\npositive predictions to the total number of positive predictions made by the model (true positives + false positives).\\nPrecision answers the question: \"Of all the samples predicted as positive, how many are actually positive?\"\\nHigher precision indicates fewer false positives.\\n\\nRecall: Recall, also known as sensitivity or true positive rate, measures the ability of the model to capture \\nall positive instances in the dataset. It is the ratio of true positive predictions to the total number of actual\\npositive instances in the dataset (true positives + false negatives). Recall answers the question: \"Of all the actual\\npositive samples, how many did the model correctly identify?\" Higher recall indicates fewer false negatives.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "'''\n",
    "Precision and recall are two important metrics used to evaluate the performance of classification models, \n",
    "\n",
    "especially in scenarios where class imbalance exists.\n",
    "\n",
    "Precision: Precision measures the accuracy of positive predictions made by the model. It is the ratio of true\n",
    "positive predictions to the total number of positive predictions made by the model (true positives + false positives).\n",
    "Precision answers the question: \"Of all the samples predicted as positive, how many are actually positive?\"\n",
    "Higher precision indicates fewer false positives.\n",
    "\n",
    "Recall: Recall, also known as sensitivity or true positive rate, measures the ability of the model to capture \n",
    "all positive instances in the dataset. It is the ratio of true positive predictions to the total number of actual\n",
    "positive instances in the dataset (true positives + false negatives). Recall answers the question: \"Of all the actual\n",
    "positive samples, how many did the model correctly identify?\" Higher recall indicates fewer false negatives.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d8618de-143b-45e7-b459-63117e075c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nROC (Receiver Operating Characteristic) curve is a graphical plot that illustrates the diagnostic ability of a\\nbinary classification model across different threshold settings. It plots the true positive rate (TPR) against the\\nfalse positive rate (FPR) at various threshold values.\\n\\nAUC (Area Under the ROC Curve) quantifies the overall performance of the model across all possible classification \\nthresholds. It represents the probability that the model will rank a randomly chosen positive instance higher than \\na randomly chosen negative instance. AUC ranges from 0 to 1, where a higher value indicates better model \\nperformance.\\n\\nROC curves and AUC are commonly used to evaluate the performance of binary classification models, especially when\\nthe class distribution is imbalanced or when the cost of false positives and false negatives varies.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "'''\n",
    "ROC (Receiver Operating Characteristic) curve is a graphical plot that illustrates the diagnostic ability of a\n",
    "binary classification model across different threshold settings. It plots the true positive rate (TPR) against the\n",
    "false positive rate (FPR) at various threshold values.\n",
    "\n",
    "AUC (Area Under the ROC Curve) quantifies the overall performance of the model across all possible classification \n",
    "thresholds. It represents the probability that the model will rank a randomly chosen positive instance higher than \n",
    "a randomly chosen negative instance. AUC ranges from 0 to 1, where a higher value indicates better model \n",
    "performance.\n",
    "\n",
    "ROC curves and AUC are commonly used to evaluate the performance of binary classification models, especially when\n",
    "the class distribution is imbalanced or when the cost of false positives and false negatives varies.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d39f0ab7-9887-4a44-b671-aced754221ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nChoosing the best metric depends on the specific goals and requirements of the problem at hand. Here are some\\nguidelines:\\n\\nConsider the Problem Context: Understand the business or application context to determine which types of errors \\n(false positives or false negatives) are more costly or impactful.\\n\\nEvaluate Trade-offs: Precision and recall have an inverse relationship, so consider the trade-offs between them. \\nChoose a metric that balances these trade-offs based on the problem requirements.\\n\\nClass Imbalance: If the dataset has a significant class imbalance, accuracy may not be an appropriate metric. \\nInstead, consider precision, recall, F1 score, or AUC-ROC, which are less sensitive to class imbalance.\\n\\nDomain Knowledge: Use domain knowledge to prioritize specific metrics that align with the goals of stakeholders or \\ndecision-makers.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "'''\n",
    "Choosing the best metric depends on the specific goals and requirements of the problem at hand. Here are some\n",
    "guidelines:\n",
    "\n",
    "Consider the Problem Context: Understand the business or application context to determine which types of errors \n",
    "(false positives or false negatives) are more costly or impactful.\n",
    "\n",
    "Evaluate Trade-offs: Precision and recall have an inverse relationship, so consider the trade-offs between them. \n",
    "Choose a metric that balances these trade-offs based on the problem requirements.\n",
    "\n",
    "Class Imbalance: If the dataset has a significant class imbalance, accuracy may not be an appropriate metric. \n",
    "Instead, consider precision, recall, F1 score, or AUC-ROC, which are less sensitive to class imbalance.\n",
    "\n",
    "Domain Knowledge: Use domain knowledge to prioritize specific metrics that align with the goals of stakeholders or \n",
    "decision-makers.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13782788-def9-4968-8b29-0ed983ff1fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMulticlass classification is a classification task where the goal is to classify instances into one of three or\\nmore classes or categories. In binary classification, there are only two classes (e.g., positive vs. negative, \\nspam vs. non-spam), while in multiclass classification, there are multiple classes (e.g., cat, dog, bird).\\n\\nIn binary classification, the output is a single probability or decision that assigns each instance to one of two \\nclasses. In contrast, in multiclass classification, the output can be a probability distribution over multiple\\nclasses, with each class receiving a probability score.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "'''\n",
    "Multiclass classification is a classification task where the goal is to classify instances into one of three or\n",
    "more classes or categories. In binary classification, there are only two classes (e.g., positive vs. negative, \n",
    "spam vs. non-spam), while in multiclass classification, there are multiple classes (e.g., cat, dog, bird).\n",
    "\n",
    "In binary classification, the output is a single probability or decision that assigns each instance to one of two \n",
    "classes. In contrast, in multiclass classification, the output can be a probability distribution over multiple\n",
    "classes, with each class receiving a probability score.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32daddb1-6473-497d-a236-e6a79950a35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q6\n",
    "'''\n",
    "Logistic regression can be extended to handle multiclass classification using various strategies, such as:\n",
    "\n",
    "One-vs-Rest (OvR): Also known as one-vs-all, this strategy involves training a separate binary logistic regression\n",
    "classifier for each class. During prediction, the class with the highest probability is chosen as the predicted \n",
    "class.\n",
    "\n",
    "Multinomial (Softmax) Logistic Regression: This approach extends logistic regression to predict the probability distribution over all classes using the softmax function. The predicted class is the one with the highest probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
