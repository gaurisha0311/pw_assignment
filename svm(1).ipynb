{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6918eafb-9414-488f-8818-19860356f315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe objective function of a linear SVM aims to \\nmaximize the margin between the support vectors of different classes while minimizing the classification error. \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "'''\n",
    "The objective function of a linear SVM aims to \n",
    "maximize the margin between the support vectors of different classes while minimizing the classification error. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56cbb6fe-21ae-4e19-87c3-ec182df0c788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe kernel trick in SVM allows the algorithm to operate in a higher-dimensional feature space without explicitly\\ncomputing the transformed feature vectors. It involves computing the dot product between feature vectors in the \\noriginal space and can be used to handle non-linear \\ndecision boundaries by mapping the data into a higher-dimensional space where it becomes linearly separable.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "'''\n",
    "The kernel trick in SVM allows the algorithm to operate in a higher-dimensional feature space without explicitly\n",
    "computing the transformed feature vectors. It involves computing the dot product between feature vectors in the \n",
    "original space and can be used to handle non-linear \n",
    "decision boundaries by mapping the data into a higher-dimensional space where it becomes linearly separable.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc1c367e-1823-4c8e-9c7e-3343c003bb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n Support vectors in SVM are the data points closest to the decision boundary (hyperplane) that define the margin.\\n They play a crucial role in determining the decision boundary and are the only points that influence the \\n position and orientation of the hyperplane. These are the data \\n points that are the most difficult to classify, and thus they provide insight into the structure of the data.\\n '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "'''\n",
    " Support vectors in SVM are the data points closest to the decision boundary (hyperplane) that define the margin.\n",
    " They play a crucial role in determining the decision boundary and are the only points that influence the \n",
    " position and orientation of the hyperplane. These are the data \n",
    " points that are the most difficult to classify, and thus they provide insight into the structure of the data.\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5338dd0a-c3ee-406c-a4b4-790a58af5df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nHyperplane: In a binary classification problem, the hyperplane is the decision boundary that separates the \\nclasses. It is a linear equation that divides the feature space into two parts. In 2D space, it's a line, \\nand in higher dimensions, it's a plane.\\n\\nMarginal Plane: The marginal plane is defined by the support vectors, which are the data points closest to the\\nhyperplane. It is parallel to the hyperplane and passes through the support vectors.\\n\\nSoft Margin: In soft-margin SVM, a margin of tolerance is allowed for misclassification. It allows some samples\\nto be on the wrong side of the margin or hyperplane, thus making the decision boundary less sensitive to \\noutliers.\\n\\nHard Margin: In contrast, a hard-margin SVM aims to find the largest possible margin without allowing any \\nmisclassifications. It is more sensitive to outliers and noise in the data.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "'''\n",
    "Hyperplane: In a binary classification problem, the hyperplane is the decision boundary that separates the \n",
    "classes. It is a linear equation that divides the feature space into two parts. In 2D space, it's a line, \n",
    "and in higher dimensions, it's a plane.\n",
    "\n",
    "Marginal Plane: The marginal plane is defined by the support vectors, which are the data points closest to the\n",
    "hyperplane. It is parallel to the hyperplane and passes through the support vectors.\n",
    "\n",
    "Soft Margin: In soft-margin SVM, a margin of tolerance is allowed for misclassification. It allows some samples\n",
    "to be on the wrong side of the margin or hyperplane, thus making the decision boundary less sensitive to \n",
    "outliers.\n",
    "\n",
    "Hard Margin: In contrast, a hard-margin SVM aims to find the largest possible margin without allowing any \n",
    "misclassifications. It is more sensitive to outliers and noise in the data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13232342-7cbe-4db2-ab6a-e7d10d971b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q6\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]  # Using only two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train linear SVM classifier\n",
    "clf = SVC(kernel='linear', C=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for testing set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='k', s=80)\n",
    "plt.xlabel('Sepal Length (cm)')\n",
    "plt.ylabel('Sepal Width (cm)')\n",
    "\n",
    "# Plot decision boundaries\n",
    "ax = plt.gca()\n",
    "xlim = ax.get_xlim()\n",
    "ylim = ax.get_ylim()\n",
    "\n",
    "# Create grid to evaluate model\n",
    "# Create grid to evaluate model\n",
    "xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 30),\n",
    "                     np.linspace(ylim[0], ylim[1], 30))\n",
    "xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "Z = clf.decision_function(xy).reshape(xx.shape)\n",
    "\n",
    "\n",
    "# Plot decision boundary and margins\n",
    "ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
    "           linestyles=['--', '-', '--'])\n",
    "ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,\n",
    "           linewidth=1, facecolors='none', edgecolors='k')\n",
    "\n",
    "plt.title(\"Linear SVM Decision Boundaries\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170bce99-f531-4c80-a1d2-fb4a49304dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
