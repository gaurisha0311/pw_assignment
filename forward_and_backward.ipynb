{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f862e361-51a3-4319-9fb5-303337364fbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nForward propagation is the process in which input data is passed through the neural network to compute the \\noutput.\\nThe purpose of forward propagation is to generate predictions or activations for each layer in the network.\\nIt involves the computation of weighted sums and activation functions for each neuron, producing the final\\noutput of the neural network.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "'''\n",
    "Forward propagation is the process in which input data is passed through the neural network to compute the \n",
    "output.\n",
    "The purpose of forward propagation is to generate predictions or activations for each layer in the network.\n",
    "It involves the computation of weighted sums and activation functions for each neuron, producing the final\n",
    "output of the neural network.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c1beb9-83ca-4aa4-9c76-7a3e79570a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nActivation functions introduce non-linearity to the model, allowing neural networks to learn complex patterns.\\nDuring forward propagation, the output of each neuron is computed by applying the activation function to the \\nweighted sum of its \\ninputs. Common activation functions include sigmoid, hyperbolic tangent (tanh), ReLU, and softmax.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "'''\n",
    "Activation functions introduce non-linearity to the model, allowing neural networks to learn complex patterns.\n",
    "During forward propagation, the output of each neuron is computed by applying the activation function to the \n",
    "weighted sum of its \n",
    "inputs. Common activation functions include sigmoid, hyperbolic tangent (tanh), ReLU, and softmax.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07e0c78c-3f07-4210-9f4d-ebbc781ea440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWeights (W) and biases (b) are the parameters of the neural network that are learned during the training process.\\nDuring forward propagation, the weighted sum of inputs, \\n\\nW⋅X+b, is computed. The weights determine the strength of connections between neurons, while biases allow the\\nmodel to shift and control the output.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "'''\n",
    "Weights (W) and biases (b) are the parameters of the neural network that are learned during the training process.\n",
    "During forward propagation, the weighted sum of inputs, \n",
    "\n",
    "W⋅X+b, is computed. The weights determine the strength of connections between neurons, while biases allow the\n",
    "model to shift and control the output.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0935a0fa-c668-4b37-84fb-333be9af7b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe softmax function is often applied in the output layer of a neural network for multi-class \\nclassification tasks. It converts the raw output scores into a probability distribution, ensuring that the sum\\nof the probabilities across all classes is equal to 1. \\nThis makes it suitable for determining the likelihood of the input belonging to each class.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "'''\n",
    "The softmax function is often applied in the output layer of a neural network for multi-class \n",
    "classification tasks. It converts the raw output scores into a probability distribution, ensuring that the sum\n",
    "of the probabilities across all classes is equal to 1. \n",
    "This makes it suitable for determining the likelihood of the input belonging to each class.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af298b33-0409-4087-ad67-79456a77d1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nBackward propagation, also known as backpropagation, is the process of updating the model's parameters (weights \\nand biases) based on the computed loss during forward propagation. It involves calculating the gradients of the \\nloss with respect to the model \\nparameters and using these gradients to update the parameters through optimization algorithms like gradient \\ndescent.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "'''\n",
    "Backward propagation, also known as backpropagation, is the process of updating the model's parameters (weights \n",
    "and biases) based on the computed loss during forward propagation. It involves calculating the gradients of the \n",
    "loss with respect to the model \n",
    "parameters and using these gradients to update the parameters through optimization algorithms like gradient \n",
    "descent.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fa11dec-12f0-4e52-8d72-c1adb158bbe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn a single-layer feedforward neural network, the gradients with respect to the parameters (weights and biases)\\nare computed using the chain rule of calculus. The mathematical expression for updating weights (\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "'''\n",
    "In a single-layer feedforward neural network, the gradients with respect to the parameters (weights and biases)\n",
    "are computed using the chain rule of calculus. The mathematical expression for updating weights (\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d9b55db-b249-4ddc-8c69-c9120a168e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe chain rule is a fundamental rule in calculus that allows us to compute the derivative of a composite \\nfunction. In the context of neural networks, the chain rule is applied to calculate the gradients \\nduring backward propagation. The chain rule states that the \\nderivative of a composition of functions is the product of the derivatives of those functions.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q8\n",
    "'''\n",
    "The chain rule is a fundamental rule in calculus that allows us to compute the derivative of a composite \n",
    "function. In the context of neural networks, the chain rule is applied to calculate the gradients \n",
    "during backward propagation. The chain rule states that the \n",
    "derivative of a composition of functions is the product of the derivatives of those functions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fef1d6-df13-4e44-82d1-14dcda4b8eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q9\n",
    "'''\n",
    "Common challenges during backward propagation include vanishing or exploding gradients, which can lead to slow \n",
    "convergence or instability during training. Techniques like weight initialization, gradient clipping, and\n",
    "using activation functions that mitigate these issues (e.g., ReLU) are commonly employed. Additionally, \n",
    "choosing appropriate learning rates and using batch normalization can help stabilize the training process. \n",
    "Regularization techniques like dropout can also be used to prevent overfitting.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
