{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f51597f-f7f6-4016-a9c1-a192d97e444e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn the context of artificial neural networks, an activation function is a mathematical operation applied to the\\noutput of each neuron in a neural network. It introduces non-linearities to the network, allowing it to learn \\ncomplex patterns in the data. The \\nactivation function decides whether a neuron should be activated or not based on the weighted sum of its inputs.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "'''\n",
    "In the context of artificial neural networks, an activation function is a mathematical operation applied to the\n",
    "output of each neuron in a neural network. It introduces non-linearities to the network, allowing it to learn \n",
    "complex patterns in the data. The \n",
    "activation function decides whether a neuron should be activated or not based on the weighted sum of its inputs.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286f9dbf-c717-4282-9959-b1cc0331a317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSome common types of activation functions used in neural networks include:\\n\\nSigmoid: Maps the input to a range between 0 and 1.\\nHyperbolic Tangent (tanh): Similar to the sigmoid but maps the input to a range between -1 and 1.\\nRectified Linear Unit (ReLU): Sets negative values to zero and passes positive values as they are.\\nLeaky ReLU: Similar to ReLU, but allows a small, non-zero gradient for negative inputs.\\nSoftmax: Used for multi-class classification, converts a vector of real numbers into a probability\\ndistribution.\\nParametric Rectified Linear Unit (PReLU): Similar to Leaky ReLU but allows the leakage rate to be learned\\nduring training.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "'''\n",
    "Some common types of activation functions used in neural networks include:\n",
    "\n",
    "Sigmoid: Maps the input to a range between 0 and 1.\n",
    "Hyperbolic Tangent (tanh): Similar to the sigmoid but maps the input to a range between -1 and 1.\n",
    "Rectified Linear Unit (ReLU): Sets negative values to zero and passes positive values as they are.\n",
    "Leaky ReLU: Similar to ReLU, but allows a small, non-zero gradient for negative inputs.\n",
    "Softmax: Used for multi-class classification, converts a vector of real numbers into a probability\n",
    "distribution.\n",
    "Parametric Rectified Linear Unit (PReLU): Similar to Leaky ReLU but allows the leakage rate to be learned\n",
    "during training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d6587b-f9e9-40fe-a279-4f3f56b3169d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nActivation functions introduce non-linearity, enabling neural networks to model complex relationships in data. \\nThey help in learning and generalization. The choice of \\nactivation function can affect convergence speed, model capacity, and the ability to handle different types of \\ndata.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "'''\n",
    "Activation functions introduce non-linearity, enabling neural networks to model complex relationships in data. \n",
    "They help in learning and generalization. The choice of \n",
    "activation function can affect convergence speed, model capacity, and the ability to handle different types of \n",
    "data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3caf0d4a-db38-4d9e-acb3-61ede8fb86c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nReLU activation function outputs the input directly if it is positive, otherwise, it outputs zero: \\n\\nf(x)=max(0,x). Unlike the sigmoid, it's not bounded,\\nallowing it to alleviate the vanishing gradient problem. ReLU is commonly used in hidden layers of neural \\nnetworks.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "'''\n",
    "ReLU activation function outputs the input directly if it is positive, otherwise, it outputs zero: \n",
    "\n",
    "f(x)=max(0,x). Unlike the sigmoid, it's not bounded,\n",
    "allowing it to alleviate the vanishing gradient problem. ReLU is commonly used in hidden layers of neural \n",
    "networks.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac38e96-ec45-47c2-a4a2-1808b3899b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBenefits of ReLU over sigmoid include:\\n\\nAddresses vanishing gradient problem, promoting better convergence.\\nComputationally more efficient.\\nEncourages sparsity in the neural network.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "'''\n",
    "Benefits of ReLU over sigmoid include:\n",
    "\n",
    "Addresses vanishing gradient problem, promoting better convergence.\n",
    "Computationally more efficient.\n",
    "Encourages sparsity in the neural network.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24e12c04-cc40-4d4e-9b05-d29471ecda88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLeaky ReLU is an extension of ReLU that allows a \\nsmall, non-zero gradient for negative inputs. It is defined as \\n\\nf(x)=max(αx,x), where \\n\\nα is a small positive constant. This prevents dead neurons (neurons that always output zero),\\naddressing the vanishing gradient problem associated with regular ReLU.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "'''\n",
    "Leaky ReLU is an extension of ReLU that allows a \n",
    "small, non-zero gradient for negative inputs. It is defined as \n",
    "\n",
    "f(x)=max(αx,x), where \n",
    "\n",
    "α is a small positive constant. This prevents dead neurons (neurons that always output zero),\n",
    "addressing the vanishing gradient problem associated with regular ReLU.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b218e191-9623-4677-b178-255f7d0028e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe softmax activation function is used in the output layer of a neural network for multi-class classification\\nproblems. It converts the raw output scores into probabilities, ensuring that the sum of the probabilities \\nacross \\nall classes is equal to 1. This makes it suitable for problems where an input can belong to one of several \\nclasses.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q8\n",
    "'''\n",
    "The softmax activation function is used in the output layer of a neural network for multi-class classification\n",
    "problems. It converts the raw output scores into probabilities, ensuring that the sum of the probabilities \n",
    "across \n",
    "all classes is equal to 1. This makes it suitable for problems where an input can belong to one of several \n",
    "classes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f75680-eadc-449b-9f36-2d6e90e60a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q9\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
