{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da47586d-5aab-4958-a832-de7644d2b00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nR-squared is a statistical measure that represents the proportion of the variance in the dependent variable that\\nis explained by the independent variables in a regression model.\\nIt is calculated as the ratio of the explained variance to the total variance of the dependent variable.\\nR-squared values range from 0 to 1, where 1 indicates a perfect fit and 0 indicates that the model does not \\nexplain any of the variability of the response data around its mean.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q1\n",
    "'''\n",
    "R-squared is a statistical measure that represents the proportion of the variance in the dependent variable that\n",
    "is explained by the independent variables in a regression model.\n",
    "It is calculated as the ratio of the explained variance to the total variance of the dependent variable.\n",
    "R-squared values range from 0 to 1, where 1 indicates a perfect fit and 0 indicates that the model does not \n",
    "explain any of the variability of the response data around its mean.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d734ae51-4afd-4cef-9d44-f49786939913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAdjusted R-squared is a modified version of R-squared that accounts for the number of predictors in the model.\\nIt penalizes the addition of unnecessary predictors that do not significantly improve the model fit.\\nAdjusted R-squared increases only if the new term improves the model more than would be expected by chance.\\nUnlike R-squared, adjusted R-squared can decrease if adding a predictor doesn't improve the model significantly.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q2\n",
    "'''\n",
    "Adjusted R-squared is a modified version of R-squared that accounts for the number of predictors in the model.\n",
    "It penalizes the addition of unnecessary predictors that do not significantly improve the model fit.\n",
    "Adjusted R-squared increases only if the new term improves the model more than would be expected by chance.\n",
    "Unlike R-squared, adjusted R-squared can decrease if adding a predictor doesn't improve the model significantly.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15954374-7c8b-41ba-a730-c93acb09958d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAdjusted R-squared is more appropriate when comparing models with different numbers of predictors.\\nIt helps to avoid overfitting by penalizing the inclusion of unnecessary predictors.\\nIt provides a more accurate measure of the goodness-of-fit of the model, especially in cases of multiple regression.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q3\n",
    "'''\n",
    "Adjusted R-squared is more appropriate when comparing models with different numbers of predictors.\n",
    "It helps to avoid overfitting by penalizing the inclusion of unnecessary predictors.\n",
    "It provides a more accurate measure of the goodness-of-fit of the model, especially in cases of multiple regression.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41c50425-fe76-4349-b58e-7c6f702e3c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRMSE: It measures the average magnitude of the errors between predicted values and actual values, taking the \\nsquare root of the mean of squared differences.\\nMSE: It measures the average of the squares of the errors.\\nMAE: It measures the average of the absolute errors between predicted values and actual values.\\nThese metrics quantify the performance of regression models by evaluating the differences between predicted and\\nactual values.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q4\n",
    "'''\n",
    "RMSE: It measures the average magnitude of the errors between predicted values and actual values, taking the \n",
    "square root of the mean of squared differences.\n",
    "MSE: It measures the average of the squares of the errors.\n",
    "MAE: It measures the average of the absolute errors between predicted values and actual values.\n",
    "These metrics quantify the performance of regression models by evaluating the differences between predicted and\n",
    "actual values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a69dda6-acb6-45aa-87d8-fc4206bf5eef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAdvantages:\\nProvide a quantitative measure of the model's accuracy.\\nEasy to interpret and understand.\\nDisadvantages:\\nSensitive to outliers, especially RMSE and MSE.\\nNot normalized, making comparison between models with different scales difficult.\\nMAE gives equal weight to all errors, which may not be desirable in some cases.\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q5\n",
    "'''\n",
    "Advantages:\n",
    "Provide a quantitative measure of the model's accuracy.\n",
    "Easy to interpret and understand.\n",
    "Disadvantages:\n",
    "Sensitive to outliers, especially RMSE and MSE.\n",
    "Not normalized, making comparison between models with different scales difficult.\n",
    "MAE gives equal weight to all errors, which may not be desirable in some cases.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e6ebc94-71a2-4e56-80e8-1b572fe1526c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLasso (Least Absolute Shrinkage and Selection Operator) regularization adds a penalty term to the linear regression\\ncost function,\\nwhich is the sum of the absolute values of the coefficients multiplied by a regularization parameter \\n\\nLasso tends to shrink the less important feature coefficients to zero, effectively performing feature selection by\\neliminating irrelevant features.\\nIt encourages sparsity in the coefficient matrix.\\nIt is more appropriate when dealing with a large number of features and when feature selection is important.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q6\n",
    "'''\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization adds a penalty term to the linear regression\n",
    "cost function,\n",
    "which is the sum of the absolute values of the coefficients multiplied by a regularization parameter \n",
    "\n",
    "Lasso tends to shrink the less important feature coefficients to zero, effectively performing feature selection by\n",
    "eliminating irrelevant features.\n",
    "It encourages sparsity in the coefficient matrix.\n",
    "It is more appropriate when dealing with a large number of features and when feature selection is important.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a67193c-1b56-4769-b2fe-156580a2a40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRegularization techniques like Lasso and Ridge add penalty terms to the regression cost function, which penalize\\nlarge coefficients.\\nBy penalizing large coefficients, regularization helps to reduce model complexity and prevent overfitting.\\nThis results in models that generalize better to unseen data.\\nFor example, in Ridge regression, the penalty term is proportional to the square of the coefficients, leading \\nto smaller coefficient values.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q7\n",
    "'''\n",
    "Regularization techniques like Lasso and Ridge add penalty terms to the regression cost function, which penalize\n",
    "large coefficients.\n",
    "By penalizing large coefficients, regularization helps to reduce model complexity and prevent overfitting.\n",
    "This results in models that generalize better to unseen data.\n",
    "For example, in Ridge regression, the penalty term is proportional to the square of the coefficients, leading \n",
    "to smaller coefficient values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9cdef1b-6f47-4e1f-a744-fb080ce58ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRegularization techniques may not perform well when there is no multicollinearity among predictors.\\nThey may not be suitable for datasets with a small number of features or when all features are equally important.\\nChoosing the appropriate regularization parameter \\n can be challenging and may require cross-validation.\\n '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#q8\n",
    "'''\n",
    "Regularization techniques may not perform well when there is no multicollinearity among predictors.\n",
    "They may not be suitable for datasets with a small number of features or when all features are equally important.\n",
    "Choosing the appropriate regularization parameter \n",
    " can be challenging and may require cross-validation.\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff51b5d-f712-4f8a-8989-c3b2abc64709",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q9\n",
    "'''\n",
    "In this case, Model B with an MAE of 8 would be considered the better performer, as it has a lower error on \n",
    "average compared to Model A with an RMSE of 10.\n",
    "However, it's essential to consider the specific characteristics of the problem and the implications of each \n",
    "metric. For example, RMSE penalizes larger errors more heavily due to the squaring operation, which may or may not be desirable depending on the context.\n",
    "It's also important to assess other aspects of the models, such as computational complexity and interpretability, before making a final decision."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
